\section{Introduction}

\subsection{Context}
The rise of machine learning in factor investing is caused by three things: data availability, computational capacity, and economic groundings.

First, the \textbf{data}. Nowadays, classical providers, such as Bloomberg and Reuters have seen their playing field invaded by niche players and aggregation platforms. Hence, firm-specific attributes are easy and often cheap to compile. This means that the size of $\mathbf{X}$ in is now sufficiently large to be plugged into ML algorithms.

Second, \textbf{computational power}, both through hardware and software. Storage and processing speed are not technical hurdles anymore and models can even be run on the cloud thanks to services hosted by major actors (Amazon, Microsoft, IBM and Google) and by smaller players (Rackspace, Techila).

Finally, \textbf{economic framing}. Machine learning applications in finance were initially introduced by computer scientists and information system experts (e.g., Braun and Chandler (1987), White (1988)) and exploited shortly after by academics in financial economics (Bansal and Viswanathan (1993)), and hedge funds (see, e.g., Zuckerman (2019)). Nonlinear relationships then became more mainstream in asset pricing (Freeman and Tse (1992), Bansal, Hsieh, and Viswanathan (1993)). These contributions started to pave the way for the more brute-force approaches that have blossomed since the 2010 decade and which are mentioned throughout the book.

\textit{In the synthetic proposal of R. Arnott, Harvey, and Markowitz (2019), the first piece of advice is to rely on a model that makes sense economically}. This sentiment is believed to be be true in the following, and the only assumption is that future returns depend on firm characteristics. The relationship between these features and performance is largely unknown and probably time-varying. This is why ML can be useful: to detect some hidden patterns beyond the documented asset pricing anomalies. Moreover, dynamic training allows to adapt to changing market conditions.

\subsection{Portfolio construction: the workflow}
The following is focused on the prediction part. Allocating to assets requires to make bets and thus to presage and foresee which ones will do well. We mostly look at \textbf{supervised learning} to forecast returns in the cross-section. The baseline equation in supervised learning
\begin{equation}
    \mathbf{y} = f(\mathbf{X}) + \epsilon 
\end{equation}
which in financial terms is
\begin{equation}
    \mathbf{r}_{t+1,n} = f(\mathbf{x}_{t,n}) + \epsilon_{t+1,n}
\end{equation}
where $f(\mathbf{x}_{t,n})$ is the expected return for time $t+1$ computed at time $t$, i.e., $\mathbb{E}_{t}[r_{t+1},n]$. The model is common to all assets share ($f$ is not indexed by $n$), similarity with panel approaches.

Building accurate predictions requires to pay attention to all terms in the above equation. Chronologically, the first step is to gather data and to process it. The only consensus is the on the $\mathbf{x}$ side the features should include classical predictors reported in the literature: market capitalization, accounting ratios, risk measures, momentum proxies, etc. For the dependent variable, many researchers and practitioners work with monthly returns, but other maturities may perform better out-of-sample.

It is tempting to think that the most important part is the choice of $f$, but its likely the choice and engineering of inputs (the variables) is just as important. Finally, the errors $\epsilon_{t+1,n}$ are often overlooked. 

\subsection{Machine learning is no magic wand}
By definition, the curse of predictions is that they rely on past data to infer patterns about subsequent fluctuations. The more or less explicit hope of any forecaster is that the past will turn out to be a good approximation of the future. Needless to say, this is a pious wish; in general, predictions fare badly.

To illustrate this sad truth, the baseline algorithms that we detail in Chapters 5 to 7 yield at best mediocre results. This is done on purpose. This forces the reader to understand that blindly feeding data and parameters to a coded function will seldom suffice to reach satisfactory out-of-sample accuracy.

Below, are some key points: 
\begin{itemize}
    \item The first point is that causality is key. If  one is able to identify $\mathbf{X} \mapsto y$, where $y$ are expected returns, then the problem is solved. Unfortunately, causality is incredibly hard to uncover.
    \item Thus, researchers have most of the time to make do with simple correlation patterns, which are far less informative and robust.
    \item Relatedly, financial datasets are extremely noisy. It is a daunting task to extract signals out of them. No-arbitrage reasonings imply that if a simple pattern yielded durable profits, it would mechanically and rapidly vanish.
    \item The no-free lunch theorem imposes that the analyst formulates views on the model. This is why economic or econometric framing is key. The assumptions and choices that are made regarding both the dependent variables and the explanatory features are decisive. As a corollary, data is key. The inputs given to the models are probably much more important than the choice of the model itself.
    \item To maximize out-of-sample efficiency, the right question is probably to paraphrase Jeff Bezos: what's not going to change? Persistent series are more likely to unveil enduring patterns.
\end{itemize}

